# -*- coding: utf-8 -*-
"""Breast Cancer Preddiction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Lagu6EZl2iY5IR1mmvZnZ5moQqDxDlr
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LeakyReLU,PReLU,ELU
from keras.layers import Dropout
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

df=pd.read_csv("/content/breast-cancer.csv")

df.head()

df.info()

df.isnull().sum()

df.describe()

df.drop(columns=['id'],inplace=True)

df['diagnosis']=df['diagnosis'].map({'M':1,'B':0}).astype(int)

sns.heatmap(df.corr())

X=df.drop(["diagnosis"],axis=1)
y=df["diagnosis"]

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)

model_1=LogisticRegression()
model_2=RandomForestClassifier(n_estimators=100)

model_1.fit(X_train,y_train)
model_2.fit(X_train,y_train)

model_1_pred=model_1.predict(X_test)
model_2_pred=model_2.predict(X_test)

m1_score=accuracy_score(y_test,model_1_pred)
m2_score=accuracy_score(y_test,model_2_pred)

print("Logistic Regression:",m1_score)
print("RandomForest:",m2_score)

param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [4, 6, 8, 10],
    'min_samples_split': [2, 5, 10]
}

grid_search_rf = GridSearchCV(estimator=model_2, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2)

grid_search_rf.fit(X, y)

# Print best parameters and best score
print("Best parameters found: ", grid_search_rf.best_params_)
print("Best score found: ", grid_search_rf.best_score_)

# Define the parameter grid
param_grid_log_reg = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}

# Set up GridSearchCV
grid_search_log_reg = GridSearchCV(estimator=model_1, param_grid=param_grid_log_reg, cv=5, n_jobs=-1, verbose=2)

# Fit GridSearchCV
grid_search_log_reg.fit(X, y)

# Print best parameters and best score
print("Best parameters found: ", grid_search_log_reg.best_params_)
print("Best score found: ", grid_search_log_reg.best_score_)

ann=Sequential()

ann.add(Dense(units=128,activation='relu',kernel_initializer='he_uniform',input_dim=30))
ann.add(Dense(units=62,activation='relu',kernel_initializer='he_uniform'))
ann.add(Dense(units=32,activation='relu',kernel_initializer='he_uniform'))
ann.add(Dense(units=16,activation='relu',kernel_initializer='he_uniform'))
ann.add(Dense(units=1,activation='sigmoid',kernel_initializer='glorot_uniform'))

ann.summary()

ann.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

model_history=ann.fit(X_train,y_train,validation_split=0.33,batch_size=10,epochs=100)

with open('model.pkl', 'wb') as file:
    pickle.dump(model_2, file)

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'])

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'])

plt.show()

